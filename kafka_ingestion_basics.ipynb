{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0c779ca-24ac-4b05-9822-bc296f5d7aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping kafka as it is not installed.\u001b[0m\u001b[33m\n",
      "Found existing installation: kafka-python 2.3.0\n",
      "Uninstalling kafka-python-2.3.0:\n",
      "  Successfully uninstalled kafka-python-2.3.0\n",
      "\n",
      "PackagesNotFoundError: The following packages are missing from the target environment:\n",
      "  - kafka\n",
      "\n",
      "\n",
      "\u001b[33mWARNING: Package(s) not found: kafka-python\u001b[0m\u001b[33m\n",
      "Collecting kafka-python\n",
      "  Using cached kafka_python-2.3.0-py2.py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/anaconda3/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached kafka_python-2.3.0-py2.py3-none-any.whl (326 kB)\n",
      "Installing collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.3.0\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ConsumerProtocolMemberMetadata_v0' from 'kafka.coordinator.protocol' (/opt/anaconda3/lib/python3.10/site-packages/kafka/coordinator/protocol.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip show kafka-python\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install kafka-python pandas\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(kafka\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/kafka/__init__.py:21\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     18\u001b[0m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\u001b[38;5;241m.\u001b[39maddHandler(NullHandler())\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconsumer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaConsumer\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconsumer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msubscription_state\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConsumerRebalanceListener\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproducer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaProducer\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/kafka/admin/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute_import\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madmin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_resource\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfigResource, ConfigResourceType\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madmin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaAdminClient\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madmin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macl_resource\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ACL, ACLFilter, ResourcePattern, ResourcePatternFilter, ACLOperation,\n\u001b[1;32m      6\u001b[0m                                       ResourceType, ACLPermissionType, ACLResourcePatternType)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madmin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnew_topic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NewTopic\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/kafka/admin/client.py:16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madmin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macl_resource\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ACLOperation, ACLPermissionType, ACLFilter, ACL, ResourcePattern, ResourceType, \\\n\u001b[1;32m     14\u001b[0m     ACLResourcePatternType, valid_acl_operations\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient_async\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaClient, selectors\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcoordinator\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConsumerProtocolMemberMetadata_v0, ConsumerProtocolMemberAssignment_v0, ConsumerProtocol_v0\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mErrors\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     IncompatibleBrokerVersion, KafkaConfigurationError, UnknownTopicOrPartitionError,\n\u001b[1;32m     20\u001b[0m     UnrecognizedBrokerVersion, IllegalArgumentError)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ConsumerProtocolMemberMetadata_v0' from 'kafka.coordinator.protocol' (/opt/anaconda3/lib/python3.10/site-packages/kafka/coordinator/protocol.py)"
     ]
    }
   ],
   "source": [
    "!pip uninstall kafka -y\n",
    "!pip uninstall kafka-python -y\n",
    "!conda remove kafka -y\n",
    "!pip show kafka-python\n",
    "!pip install kafka-python pandas\n",
    "\n",
    "\n",
    "import kafka\n",
    "print(kafka.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3767d052-b773-43d4-b73a-c78fea9e62a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ConsumerProtocolMemberMetadata_v0' from 'kafka.coordinator.protocol' (/opt/anaconda3/lib/python3.10/site-packages/kafka/coordinator/protocol.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaProducer, KafkaConsumer\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/kafka/__init__.py:21\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     18\u001b[0m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\u001b[38;5;241m.\u001b[39maddHandler(NullHandler())\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconsumer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaConsumer\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconsumer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msubscription_state\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConsumerRebalanceListener\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproducer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaProducer\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/kafka/admin/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute_import\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madmin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_resource\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfigResource, ConfigResourceType\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madmin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaAdminClient\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madmin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macl_resource\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ACL, ACLFilter, ResourcePattern, ResourcePatternFilter, ACLOperation,\n\u001b[1;32m      6\u001b[0m                                       ResourceType, ACLPermissionType, ACLResourcePatternType)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madmin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnew_topic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NewTopic\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/kafka/admin/client.py:16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madmin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macl_resource\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ACLOperation, ACLPermissionType, ACLFilter, ACL, ResourcePattern, ResourceType, \\\n\u001b[1;32m     14\u001b[0m     ACLResourcePatternType, valid_acl_operations\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient_async\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaClient, selectors\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcoordinator\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConsumerProtocolMemberMetadata_v0, ConsumerProtocolMemberAssignment_v0, ConsumerProtocol_v0\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mErrors\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     IncompatibleBrokerVersion, KafkaConfigurationError, UnknownTopicOrPartitionError,\n\u001b[1;32m     20\u001b[0m     UnrecognizedBrokerVersion, IllegalArgumentError)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ConsumerProtocolMemberMetadata_v0' from 'kafka.coordinator.protocol' (/opt/anaconda3/lib/python3.10/site-packages/kafka/coordinator/protocol.py)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from kafka import KafkaProducer, KafkaConsumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f81e19e-a306-4800-952b-cd37f91c381c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# Kafka Producer — Full Code + Explanation\n",
    "# ------------------------------------------\n",
    "\n",
    "# Import KafkaProducer class from kafka-python library\n",
    "# This class allows you to send messages to Kafka topics.\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "# JSON library used to convert Python dict -> JSON string -> bytes\n",
    "import json\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CREATE A KAFKA PRODUCER\n",
    "# ---------------------------------------------------------\n",
    "producer = KafkaProducer(\n",
    "    # Hostname & port of Kafka Broker.\n",
    "    # Producer connects to this broker, discovers all brokers in cluster.\n",
    "    bootstrap_servers='localhost:9092',\n",
    "\n",
    "    # Convert Python dict → JSON string → bytes,\n",
    "    # because Kafka messages must be bytes.\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "\n",
    "    # Convert string keys into bytes (Kafka stores keys as bytes).\n",
    "    key_serializer=lambda k: k.encode('utf-8') if k else None\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# SEND MESSAGES\n",
    "# ---------------------------------------------------------\n",
    "for i in range(5):\n",
    "\n",
    "    # Create a Python dictionary to send as JSON\n",
    "    msg = {\"id\": i, \"value\": f\"message-{i}\"}\n",
    "\n",
    "    # Send to topic \"test_topic\"\n",
    "    # Internally:\n",
    "    # 1. Key decides which partition the message goes to\n",
    "    # 2. Value is serialized into bytes\n",
    "    # 3. Message sits in a local buffer (batch)\n",
    "    producer.send(\"test_topic\", key=str(i), value=msg)\n",
    "\n",
    "    # Print for our reference\n",
    "    print(\"Sent:\", msg)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# FLUSH AND CLOSE\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# flush(): forces all buffered (batched) messages to be delivered to Kafka\n",
    "# KafkaProducer batches messages to improve throughput\n",
    "producer.flush()\n",
    "\n",
    "# close(): closes TCP connection + flushes leftover messages\n",
    "producer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6ec3bb-ddee-423f-8f8c-9ca3782dcdbd",
   "metadata": {},
   "source": [
    "Behind-the-scenes (Important Explanation)\n",
    "1. KafkaProducer internal workflow\n",
    "When you create:\n",
    "KafkaProducer(...)\n",
    "Kafka does the following in background:\n",
    "Opens a TCP connection to the Kafka Broker (localhost:9092)\n",
    "Gets metadata about:\n",
    "All topics\n",
    "All partitions\n",
    "Leaders for each partition\n",
    "Initializes a background I/O thread that:\n",
    "Batches messages\n",
    "Compresses (if configured)\n",
    "Sends messages asynchronously\n",
    "2. Serializers\n",
    "Kafka messages must be bytes.\n",
    "That means Python objects must be converted → bytes.\n",
    "value_serializer\n",
    "Converts dict → JSON → bytes\n",
    "key_serializer\n",
    "Converts string key → bytes\n",
    "Kafka uses key to decide the partition:\n",
    "Same key = same partition\n",
    "Used for ordering guarantees\n",
    "3. producer.send()\n",
    "When you send:\n",
    "producer.send(\"test_topic\", key=str(i), value=msg)\n",
    "Kafka does NOT immediately send it.\n",
    "Instead:\n",
    "Message is placed in an in-memory batch buffer\n",
    "KafkaProducer background thread sends batches to broker\n",
    "Broker writes message into:\n",
    "Topic\n",
    "Partition\n",
    "Commit log (append-only file)\n",
    "4. producer.flush()\n",
    "Flush forces:\n",
    "send all batched messages\n",
    "wait for broker ACK\n",
    "guaranteed delivery before closure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d458a75e-64b6-43b6-b207-352c097e0c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# Kafka Consumer — Full Code + Explanation\n",
    "# ------------------------------------------\n",
    "\n",
    "# Import KafkaConsumer class\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "# JSON for converting byte → JSON → Python dict\n",
    "import json\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CREATE CONSUMER\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    # Topic to subscribe to\n",
    "    \"test_topic\",\n",
    "\n",
    "    # Where Kafka Consumer should connect\n",
    "    bootstrap_servers='localhost:9092',\n",
    "\n",
    "    # If no previous offset is stored, start reading from:\n",
    "    # 'earliest' -> beginning of topic\n",
    "    # 'latest'   -> newest messages only\n",
    "    auto_offset_reset='earliest',\n",
    "\n",
    "    # Automatically commit offsets periodically\n",
    "    # Kafka stores consumer's last read position in \"__consumer_offsets\"\n",
    "    enable_auto_commit=True,\n",
    "\n",
    "    # Consumers reading together share a GROUP ID\n",
    "    # Kafka assigns partitions among consumers in the same group\n",
    "    group_id=\"group1\",\n",
    "\n",
    "    # Convert bytes → JSON string → Python dict\n",
    "    value_deserializer=lambda v: json.loads(v.decode('utf-8'))\n",
    ")\n",
    "\n",
    "print(\"Waiting for messages...\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# READ MESSAGES (Infinite Loop)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "for message in consumer:\n",
    "\n",
    "    # message.key is in bytes, message.value is Python dict (because we deserialized)\n",
    "    print(f\"Key={message.key}, Value={message.value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cded0c-8f2d-4671-8ec1-91ddb99dcb2d",
   "metadata": {},
   "source": [
    "When you create KafkaConsumer()\n",
    "Kafka does the following:\n",
    "Connects to broker on localhost:9092\n",
    "Joins consumer group group1\n",
    "Broker decides which partition this consumer will read\n",
    "Fetches offsets from internal Kafka topic __consumer_offsets\n",
    "2. Consumption logic\n",
    "For each new message:\n",
    "Kafka stores messages in a commit log\n",
    "Consumer polls (pull-based)\n",
    "Consumer receives a batch of messages\n",
    "Deserializer converts bytes → Python object\n",
    "3. auto_offset_reset='earliest'\n",
    "Used only when no committed offset exists.\n",
    "Meaning:\n",
    "First time consumer runs → starts from beginning\n",
    "Next runs → continues from stored offset\n",
    "4. enable_auto_commit=True\n",
    "Consumer stores offsets automatically every 5 seconds.\n",
    "Offsets are stored in Kafka topic:\n",
    "__consumer_offsets\n",
    "This ensures consumers can continue where they left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a1b324-7486-46e3-8160-cc40d7475f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# INGESTING DATA FROM FILE (CSV → Kafka)\n",
    "# ------------------------------------------\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Create Kafka Producer\n",
    "# ----------------------------------------------------\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='localhost:9092',                 # Connect to broker\n",
    "    value_serializer=lambda v: json.dumps(v).encode()  # Convert dict -> bytes\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Load CSV File into a DataFrame\n",
    "# ----------------------------------------------------\n",
    "df = pd.read_csv(\"employees.csv\")     # Example file: name, age, dept\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Iterate each row and send as JSON to Kafka\n",
    "# ----------------------------------------------------\n",
    "for _, row in df.iterrows():\n",
    "\n",
    "    record = row.to_dict()            # Convert row -> dict format\n",
    "    producer.send(\"file_topic\", value=record)  \n",
    "    # Behind the scenes:\n",
    "    # 1. Row converted to JSON bytes\n",
    "    # 2. Added to producer buffer\n",
    "    # 3. Background thread batches & sends to Kafka\n",
    "    print(\"Sent:\", record)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Make sure all messages are pushed\n",
    "# ----------------------------------------------------\n",
    "producer.flush()\n",
    "producer.close()\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# INGESTING DATA FROM API → Kafka\n",
    "# ------------------------------------------\n",
    "\n",
    "import requests\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Create Kafka Producer\n",
    "# ----------------------------------------------------\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_serializer=lambda v: json.dumps(v).encode()\n",
    ")\n",
    "\n",
    "# API endpoint to get data from\n",
    "API_URL = \"https://api.example.com/users\"  \n",
    "# Replace with real API\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Poll API and send each item to Kafka\n",
    "# ----------------------------------------------------\n",
    "while True:       # Keep running every interval\n",
    "\n",
    "    response = requests.get(API_URL)\n",
    "    data = response.json()            # Convert API response -> Python object\n",
    "\n",
    "    for item in data:\n",
    "        producer.send(\"api_topic\", value=item)\n",
    "        print(\"Sent:\", item)\n",
    "\n",
    "    # Wait 5 seconds before next API call\n",
    "    time.sleep(5)\n",
    "\n",
    "# ------------------------------------------\n",
    "# INGESTING FROM DATABASE → Kafka\n",
    "# ------------------------------------------\n",
    "\n",
    "import mysql.connector\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Create Kafka Producer\n",
    "# ----------------------------------------------------\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_serializer=lambda v: json.dumps(v).encode()\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# MySQL Database Connection\n",
    "# ----------------------------------------------------\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"password\",\n",
    "    database=\"company\"\n",
    ")\n",
    "\n",
    "cursor = conn.cursor(dictionary=True)    \n",
    "# dictionary=True -> returns rows as dict (easier to convert to JSON)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Periodically read table and push new rows\n",
    "# ----------------------------------------------------\n",
    "while True:\n",
    "    cursor.execute(\"SELECT * FROM employees\")   # Fetch table rows\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    for row in rows:\n",
    "        producer.send(\"db_topic\", value=row)\n",
    "        print(\"Sent:\", row)\n",
    "\n",
    "    time.sleep(10)   # Poll DB every 10 sec (simple incremental ingestion)\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# INGESTING DATA FROM CLOUD STORAGE (AWS S3 → Kafka)\n",
    "# ------------------------------------------\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Create Kafka Producer\n",
    "# ----------------------------------------------------\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_serializer=lambda v: json.dumps(v).encode()\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Connect to AWS S3\n",
    "# ----------------------------------------------------\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=\"YOUR_KEY\",\n",
    "    aws_secret_access_key=\"YOUR_SECRET\"\n",
    ")\n",
    "\n",
    "bucket = \"my-bucket\"\n",
    "file_name = \"data.json\"\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Download file content from S3\n",
    "# ----------------------------------------------------\n",
    "obj = s3.get_object(Bucket=bucket, Key=file_name)\n",
    "content = obj[\"Body\"].read().decode(\"utf-8\")     # Read file as string\n",
    "data = json.loads(content)                        # Convert -> Python list/dict\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Send each record to Kafka\n",
    "# ----------------------------------------------------\n",
    "for record in data:\n",
    "    producer.send(\"cloud_topic\", value=record)\n",
    "    print(\"Sent:\", record)\n",
    "\n",
    "producer.flush()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
